<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[LambdaMART Derivation]]></title>
      <url>/2018/09/01/LambdaMART-Derivation/</url>
      <content type="text"><![CDATA[Last time we talked about the general workflow of learning to rank and how to implement LambdaMART in python. This time, we will go through the derivations of LambdaMART.RankNetRankNet maps an input feature vector $x\in \mathbb{R}^N$ to a number $f(x)$. For each pair of documents with different labels $d_i$ and $d_j$, we can obtain a score $s_i=f(x_i)$ and $s_j=f(x_j)$. Let $d_i\rhd d_j$ denotes the event that $d_i$ is ranked above $d_j$. We can map the scores $s_i$ and $s_j$ to a learned probability model such that $d_i$ should be ranked higher than $d_j$:We define the cost function using cross entropywhere $\bar{P}_{ij}$ is the known probability that $d_i\rhd d_j$.Let $S_{ij}\in{0, \pm 1}$ be 1 if $d_i\rhd d_j$, -1 if $d_j\rhd d_i$, and 0 if $d_i$ and $d_j$ have the same rank. For simplicity, we assume that the ranking is deterministic known from labelled data. Thus, we have $\bar{P}{ij}=\frac{1}{2}(1+S{ij})$. Re-write the cost function, we haveTo update the weights of the model, we have the familiar gradient descent formulawhereNote thatNow, we sub Eq.(2) into Eq.(1), we can factorize the cost function to the following form:where This approach works well if the cost function is differentiable with respect to $s_i$. However, most evaluation metrics in information retrieval such as NDCG are discontinuous as many of them need to sort the result before computing the actual value. As a result, we cannot directly use gradient descent to update the model parameters.LambdaRankThe problem with RankNet is that it could not optimize information retrieval metrics directly. Here is a graphical illustration of the issue.In the graph above, each grey line represents an irrelevant document, while blue line represents a relevant document. In the left, the number of pairwise errors is 13. By moving the relevant document at the top down by 3 and the bottom document up by 5, we can reduce the number of errors to 11. This is represented by the black arrows. However, for most information retrieval metrics, we only care about the top $k$ entries and the gradients are represented by red arrows.Is there any chance we can obtain the gradients (red arrows) directly? Based on empirical results, modifying Eq.(3) by scaling the size of change of evaluation metrics (such as NDCG and MAP) which we will denote by $\lvert\Delta Z_{ij}\rvert$ given by swapping rank position of $d_i$ and $d_j$ gives good results. As a result, in LambdaRank we assume there is a utility function $U$ whose derivative is $\lambda_{ij}$ such thatHere we need to use utility function instead of cost function because for most information retrieval metrics, higher means better, so we would like to maximize it. As a result, the weight update rule becomesLet $I$ denote the set of pairs ${i, j}$ such that $d_i\rhd d_j$. For a given query, $\lambda_i$ associated with each $d_i$ is thereforeThus for each $d_i$, we can write down a utility functionPhysical Interpretation of LambdaRankIn the original paper, the authors gave a physical interpretation of LambdaRank which is noteworthy. The authors suggested that we can essentially treat each document as a point mass and $\lambda$-gradients are forces on the point mass. Positive lambda represents a push toward the top rank while negative lambda represents a push toward lower rank. We can therefore compute the net force acting on the point mass and the changes in the magnitude of the forces during training.MARTBefore going over LambdaMART, it is worthwhile to take a quick detour to MART. The complete name for MART is Multiple Additive Regression Tree, which as its name suggests, essentially applies gradient boosting to regression trees. In a general supervised learning problem, we are trying to find an approxiamtion $\hat{f}(x)=y$ such thatwhere $L(\cdot, \cdot)$ denotes loss funtion.Under the settings of MART, we are looking forwhere $T(x; \Theta_m)$ denotes a tree model with prameters $\Theta_m$.With stage-wise additive modeling (for more details, refer to Elements of Statistical Learning Chapter 10), at each step in the forward step-wise procedure, we haveSolving the above equation exactly is quite challenging. However, we realize that this is a greedy approach. At each step, we would like $T(x_i; \Theta_m)$ to minimize Eq.(5) given $f_{m-1}$ fitted on $x_i$. Thus, $T(x_i; \Theta_m)$ is analogous to the negative gradient defined byAs a result, we can now solve Eq.(5) withFrom Eq.(6), we can see that for MART model, we do not really need to define a specific loss function. In fact, all we care about is the gradient $g_{im}$.LambdaMARTNow, we can see that      MART is a general framework that only requires a gradient to work        LambdaRank defines a gradient  LambdaRank and MART naturally pairs up which gives us LambdaMART.From Eq.(4), we can obtainwhere $\rho_{ij}\defeq\frac{1}{1+e^{\sigma(s_i-s_j)}}$.Then we can also obtainThe Newton step-size for the $k$-th leaf of the $m$-th tree isThe overall procedure of LambdaMART is therefore given byReferences      From RankNet to LambdaRank to LambdaMART: An Overview        On the Local Optimality of LambdaRank        Adapting Boosting for Information Retrieval Measures        Learning to Rank with Nonsmooth Cost Functions        A Not So Simple Introduction to LambdaMART  ]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> machine learning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Learning to Rank - Overview]]></title>
      <url>/2018/08/30/Learning-to-Rank/</url>
      <content type="text"><![CDATA[Learning to Rank (LTR) is essentially applying supervised machine learning to ranking problems. For ranking problem, we are usually given a set of items and our objective is to find the optimal ordering of this set. For example, in web search, we need to find the best ordering of websites such that websites most relevant to the given query are ordered in the front.For the purpose of this note, I will give a quick overview of the LTR framework and provide a demo on how to actually solve a LTR problem in Python with LambdaMART, which is one of the most successful LTR methods.LTR WorkflowSimilar to other supervised learning, we need a training set. In tasks like classification or regression, a typical training observation has the form of $(x_i, y_i)$ pair with $x_i$ denotes its feature and $y_i$ denotes the corresponding value we are trying to predict. However, LTR is different in the set-up because usually we need to order the results based on a particular input. Using the example of web search again, the optimal ordering of search results depends on what the user is actually looking for. With different queries, the ordering of websites should be quite different. As a result, training set of LTR tasks has the following form:where $q_i$ denotes the query, $D_i$ is an ordered set containing documents $d_{ij}$ together with their relevance score $y_{ij}$ given query $q_i$.Once training set is ready, we can train a specific algorithm to learn a ranking model so that for an unseen query and its associated documents so that we can predict the ranking of the documents based on the metric (i.e. loss function) defined when we are training the ranking model.Example with PythonHere is a quick example of how to use LTR in Python using xgboost. As an alternaive, you can also use lightgbm which has similar interface.First, we need to download the dataset from here. Once download finishes, extract the files and copy train.txt, test.txt and vali.txt from Fold1 to a desired folder.Next, download the data processing script called trans_data.py from here. Make sure to place it in the same folder as train.txt, test.txt and vali.txt. Now open terminal and run the following command:python trans_data.py train.txt mq2008.train mq2008.train.grouppython trans_data.py test.txt mq2008.test mq2008.test.grouppython trans_data.py vali.txt mq2008.vali mq2008.vali.groupThis will get us the features as well as group information required to perform LTR tasks.Now in Python, we first load all the required packages and data.import xgboost as xgbfrom xgboost import DMatrixfrom sklearn.datasets import load_svmlight_file# load featurex_train, y_train = load_svmlight_file("mq2008.train")x_valid, y_valid = load_svmlight_file("mq2008.vali")x_test, y_test = load_svmlight_file("mq2008.test")# load group informationgroup_train = []with open("mq2008.train.group", "r") as f:    data = f.readlines()    for line in data:        group_train.append(int(line.split("\n")[0]))group_valid = []with open("mq2008.vali.group", "r") as f:    data = f.readlines()    for line in data:        group_valid.append(int(line.split("\n")[0]))group_test = []with open("mq2008.test.group", "r") as f:    data = f.readlines()    for line in data:group_test.append(int(line.split("\n")[0]))For xgboost, we need to transform data into a built-in data structure called DMatrix before training the model.train_dmatrix = DMatrix(x_train, y_train)valid_dmatrix = DMatrix(x_valid, y_valid)test_dmatrix = DMatrix(x_test)# set group informationtrain_dmatrix.set_group(group_train)valid_dmatrix.set_group(group_valid)Now training and get prediction from the model will be fairly straightforward. It is similar to classification or regression, except that LTR need an extra group information (which we have incorporated into DMatrix in the previous step).params = {'objective': 'rank:pairwise', 'eta': 0.1, 'gamma': 1.0,               'min_child_weight': 0.1, 'max_depth': 6}xgb_model = xgb.train(params, train_dmatrix, num_boost_round=4,                      evals=[(valid_dmatrix, 'validation')])pred = xgb_model.predict(test_dmatrix)Note that the predicted results give us a score for ordering the test set. Thus before applying any evaluation metric, we need to sort the test set based on the scores first.]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> machine learning </tag>
        
          <tag> python </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Installing Ubuntu on MacBook Pro]]></title>
      <url>/2018/08/28/Installing-Ubuntu-on-MacBook-Pro/</url>
      <content type="text"><![CDATA[This posts records the steps I have taken to install Linux Mint 18.3 on MacBook Pro 11, 3 as well as some of the errors I have encoutnered with solutions I found online. The steps should be almost identical for installing Ubuntu.Install Linux MintHonestly this is the easiest step. In short, one just have to:      partition the disk in OS X using Disk Utility        create a bootable USB stick        install Linux Mint  More detailed steps can be found here.A few caveats regarding the tutorial given by the link above based on my experience:      if you have windows machine, Linux Live USB Creator can be a very handy tool to create bootable USB stick        during installation, you may want to make sure you are connected to the internet. To do that, once you boot into the USB stick, open driver manager and install the wifi driver. This is similar to step 6.2 in the tutorial.  Install NVIDIA Drivers on MacBook ProOnce you are done installing the system and set up internet connection, you can start working on this. The following steps should help you getting the latest official drivers from NVIDIA. First, open your terminal and entersudo add-apt-repository ppa:graphics-drivers/ppasudo apt updateThen open drivers manager, you should be able to see the latest drivers available. Although it is possible to do all these in terminal, to save the trouble, I would recommend just using the drivers manager.Once you are done installing the drivers from drivers manager, the tough fight starts.Adjust Brightness after Installing NVIDIA DriverA common problem one encounters after installing NVIDIA drivers is not able to adjust brightness with function keys. To solve this problem, open your terminal, entersudo nano /etc/X11/xorg.confand copy the followingSection "Screen"	Identifier	"Default Screen"	DefaultDepth	24EndSectionSection "Module"	Load	"glx"EndSectionSection "Device"	Identifier	"Default Device"	Driver	"nvidia"	Option	"NoLogo"	"True"	Option "RegistryDwords" "EnableBrightnessControl=1"EndSectionExit and save. Most tutorials would tell you a similar step when you are working with NVIDIA on Ubuntu. However, to make it work on MacBook, you need an extra step. Now, in the terminal, entersetpci -v -H1 -s 00:01.00 BRIDGE_CONTROL=0Usually, it is a good idea to write this line to /etc/rc.local so it is executed when the system starts.Using Intel Graphics Card in Ubuntu on MacBook ProEven if one has installed nvidia-prime, it is not possible to switch video card on a MacBook, because Intel’s graphic card is not yet acessible to Ubuntu. To overcome this problem, we need a hack called apple_set_os.efi which is available here.Here are the steps to make it work on your MacBook adapted from the write-up suggested by Andrey.1. Build apple_set_os.efiThe author listed a few methods to build it. I would recommend build it from source and NOT use the relseased ones since they can be quirky.apt-get install gnu-efigit clone https://github.com/0xbb/apple_set_os.eficd apple_set_os.efimake2. Copy to EFI partitionsudo cp apple_set_os.efi /boot/efi/EFI/3. Add a new menu entry to Grub boot loaderOpen terminal and entersudo nano /etc/grub.d/40_customand copy the followingmenuentry "Unlock Intel IGD" {  search --no-floppy --set=root --file /EFI/apple_set_os.efi  chainloader /EFI/apple_set_os.efi  boot}Then open /etc/default/grub file and make sure they look likeGRUB_DEFAULT=0# GRUB_HIDDEN_TIMEOUT=0# GRUB_HIDDEN_TIMEOUT_QUIET=trueGRUB_TIMEOUT=10GRUB_CMDLINE_LINUX="acpi_backlight=intel_backlight"Now, run sudo update-grub4. Switch to Intelgit clone https://github.com/0xbb/gpu-switchcd gpu-switch/sudo ./gpu-switch -i5. RebootOnce your MacBook reboot, before selecting the actual OS such Ubuntu or Mint, remember to select Unlock Intel IGD first. If not, you may see a flickering screen and have to for power down.Once you are in the system, you can verify if your Intel graphic is activated by$ lspci | grep "VGA"00:02.0 VGA compatible controller: Intel Corporation Crystal Well Integrated Graphics Controller (rev 08)01:00.0 VGA compatible controller: NVIDIA Corporation GK107M [GeForce GT 750M Mac Edition] (rev a1)$ glxinfo | grep "OpenGL renderer"OpenGL renderer string: Mesa DRI Intel(R) Haswell MobileOnce everythin works out correctly, you can skip the step of selecting Unlock Intel IGD by writing the following to /etc/grub.d/40_customsearch --no-floppy --set=root --file /EFI/apple_set_os.efichainloader /EFI/apple_set_os.efibootExtra Steps for OverheatingThe main motiavation behind using Intel graphics instead of NVIDIA is to reduce heat and enhance battery life. During my search online, I found a few other methods that could potentially help to reduce overheating.1. tlpTLP is a power management tool on Linux. To install,sudo add-apt-repository ppa:linrunner/tlpsudo apt-get updatesudo apt-get install tlp tlp-rdw2. cpufreqThis tool allows you to run your laptop in two modes, which are performance and powersave. I did not test how much energy it could save, just including it here for reference.sudo apt-get install indicator-cpufreqNote that with newer Core processors, you will not be able to choose specific frequency with cpufreq as some screen shots you can find online. For more details, please refer to this SE thread.3. mbpfanThis package uses the temperature from coretemp module as input and adjusts the fan speed accordingly. The installation is not very straightforward, but the author has provided a easy-to-follow step-by-step instruction here.Referenceshttp://www.daveoncode.com/2015/05/26/installing-and-configuring-linux-mint-in-dual-boot-on-an-apple-macbook-pro-with-retina-display/https://iocrunch.com/2014/08/nvidia-backlight-in-linux-on-macbookpro-113/https://github.com/Dunedan/mbp-2016-linux/issues/6https://itsfoss.com/reduce-overheating-laptops-linux/https://blog.csdn.net/redstone0001/article/details/17042011https://askubuntu.com/questions/544266/why-are-missing-the-frequency-options-on-cpufreq-utils-indicatorhttps://ineed.coffee/3838/a-beginners-tutorial-for-mbpfan-under-ubuntu/]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> Linux </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Homemade sklearn Classifier]]></title>
      <url>/2017/11/12/SKLearn-Compatible-Classifier/</url>
      <content type="text"><![CDATA[sklearn offers a variety of tools which allows quick set-up of machine learning pipelines, and many other packages are also compatible with sklearn api. When there comes an occasion you have to implement your own algorithm, it is a good idea to make it the sklearn way (so you can leave hyper-parameter tuning to bayes_opt)The StepsThere are four essential components of a sklearn classifier: get_params, set_params, fit and predict. The first two components can be easily accomplished by inheriting from the BaseEstimator class from sklearn.base. For classifiers, you should also inherit from ClassifierMixin to get support for model_selection.GridsearchCV and model_selection.cross_val_score. Other supported types for machine learning are RegressorMixin and ClusterMixin.I realized that there are quite a few posts on this topic, and I think the one by Daniel is great, so I will not repeat the details here. In addition, the official document also contains some instructions on the sklearn standard.An ExampleThe algorithm here was taken from part of my undergraduate project. In retrospect, there are a lot of areas for improvements, but I will leave it as it is. The code below is basically a simple implementation of this paper. The algorithms is a little bit involved. I will try to add a simple explanation later.import numpy as npimport scipyfrom scipy.spatial.distance import pdist, squareformfrom sklearn.base import BaseEstimator# helper function for graph Laplacian# returns normalized graph Laplaciandef _graph_laplacian(input_vector, tau):        # This computes the graph laplacian according to equation 2.5        data_size = input_vector.shape[0]    pairwise_sq_dists = squareform(pdist(input_vector, 'sqeuclidean'))    L = -scipy.exp(-pairwise_sq_dists / tau) + np.eye(data_size)    for k in range(data_size):        L[k, k] = -np.sum(L[k, :])    # compute inverse square root of diagonal matrix    d = np.diag(L)    D = np.mat(np.diag(d))    D_inv_sqrt = np.zeros(D.shape)    np.mat(np.fill_diagonal(D_inv_sqrt, 1 / (np.array(D.diagonal())[0] ** 0.5)))    # compute normalized graph laplacian by equation 2.7    Ls = D_inv_sqrt.dot(L).dot(D_inv_sqrt)    return Ls# helper function for convexsplitdef _convexsplit(eigenvectors, eigenvalues, u0, train_size, c=1, epsilon=2, maxit=50):        #This method performs the convexsplit scheme to minimize energy potential    # eigenvectors is a matrix whose columns are eigenvectors Phi_i    # eigenvalues is an array whose entries are corresponding eigenvalues Lambda_i    # here we deal with the case when eigenvectors is a square matrix    dt = 0.1    k = eigenvalues.size    n = u0.shape[0]    # initialize matrices for storage    # this part can be optimized    # since we do not need to store the result from each iteration    A = np.zeros([k, maxit + 1])    B = np.zeros([k, maxit + 1])    D = np.zeros([k, maxit + 1])    F = np.zeros(k)    U = np.zeros([n, maxit + 1])    # initialize u_0 and lambda    # gamma here denotes lambda in the original formula    U[:, 0] = np.zeros(n)    U[0: train_size, 0] = u0[0: train_size].transpose()    gamma = np.zeros(n)    gamma[0: train_size] = np.ones(train_size)    # decompose u0 u0.^3 as sums of Phi_i    # where 0 &lt;= i &lt;= k-1    a1 = np.linalg.solve(eigenvectors, U[:, 0])    u0_cube = np.array(U[:, 0]) ** 3    b1 = np.linalg.solve(eigenvectors, u0_cube)    A[:, 0] = a1    B[:, 0] = b1    F = 1 + dt * (epsilon * eigenvalues + c)    # convexsplit iteration    for j in range(maxit):        A[:, j + 1] = (F ** -1) * ((1 + dt / epsilon + c * dt) \                        * A[:, j] - dt / epsilon * B[:, j] - dt * D[:, j])        U[:, j + 1] = eigenvectors.dot(A[:, j + 1])        B[:, j + 1] = np.linalg.solve(eigenvectors, U[:, j + 1] ** 3)        D[:, j + 1] = np.linalg.solve(eigenvectors,                         np.multiply(gamma, U[:, j + 1] - U[:, 1]))    return U[:, maxit]class ConvexsplitClassifier(BaseEstimator, ClassifierMixin):    # This is the sklearn style interface        def __init__(self, tau=0.3, c=1, epsilon=2, maxit=500):        # Initialize classifier                self.tau = tau        self.c = c        self.epsilon = epsilon        self.maxit = maxit        self.x_train = None        self.y_train = None    def fit(self, x_train, y_train):        # For a standard fit method,        # you should verify all inputs are valid here        self.x_train = x_train        self.y_train = np.copy(y_train)        self.y_train[self.y_train == 0] = -1 # negative cases are labelled as -1    def predict(self, x_test):        train_size = self.x_train.shape[0]        # combine train set with test set        X = np.concatenate((self.x_train, x_test), axis=0)                # compute laplacian of combined data        L = _graph_laplacian(X, tau=self.tau)                # compute eigenvalues and eigenvectors of laplacian        Lam, Phi = np.linalg.eig(L)                # initialize u0        u0 = np.zeros(X.shape[0])        u0[0:train_size] = self.y_train        y_pred = _convexsplit(Phi, Lam, u0, train_size)[train_size:]        # sign(u) gives the predicted class        # format the output to 0 and 1        y_pred[y_pred &lt;= 0] = 0        y_pred[y_pred &gt; 0] = 1        return y_pred]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> python </tag>
        
          <tag> machine learning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Stacking with Bayesian Optimization]]></title>
      <url>/2017/11/08/Stacking-Template/</url>
      <content type="text"><![CDATA[IntroductionStacking has become a common practice in machine learning competitions such as Kaggle. Here is a simple stacking template based on the model I used in Alibab Tianchi competition. This template incorporates bayesian optimization, which will save you the time spent on grid searching hyper-parameters. I have uploaded my template notebook here.As a disclaimer, since I am using random data generated by sklearn, the performance of the stacking model may vary. Again, since this is just for illustration, I copied the parameter range from my original model, which may not be optimal. In addition, to obtain decent results from bayesian optimization, you should increase the number of iterations and combine with your experience when setting the range for hyper-parameters.import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pyplot as pltimport warningswarnings.filterwarnings('ignore')%matplotlib inlinepd.set_option('display.max_columns', None)Load Datafrom sklearn.datasets import make_classificationfrom sklearn.model_selection import train_test_splitfrom sklearn.cross_validation import KFold# generate random samples for inllustration# we will work with an imbalanced data set X, y = make_classification(n_samples=5000,                            n_classes=2,                            n_features=10,                            n_informative=5,                            weights=[0.95, 0.05],                           random_state=42)x_train, x_test, y_train, y_test = \                train_test_split(X, y, random_state=42, test_size=0.2)Stacking Preparations# Set up parametersntrain = x_train.shape[0]ntest = x_test.shape[0]SEED = 42 NFOLDS = 5 kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)There are quite a few ways to train meta-features for stacking model. I have tried a few and eventually decided to stay with the one mentioned by Anisotropdc, which is to compute predictions on test set using base model trained on each fold and then take average. Based on my experience, this method gives the best performance comparing to other simple stacking methods. If you are interested in fancier methods, you may want to take a look here.# Function to obtain out-of-fold predictiondef get_oof(clf, x_train, y_train, x_test):    oof_train = np.zeros((ntrain,))    oof_test = np.zeros((ntest,))    oof_test_skf = np.empty((NFOLDS, ntest))    for i, (train_index, test_index) in enumerate(kf):        x_tr = x_train[train_index]        y_tr = y_train[train_index]        x_te = x_train[test_index]        clf.fit(x_tr, y_tr)        oof_train[test_index] = clf.predict(x_te)        oof_test_skf[i, :] = clf.predict(x_test)    oof_test[:] = oof_test_skf.mean(axis=0)    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)Tuning Base Modelsfrom sklearn.model_selection import cross_val_scorefrom sklearn.metrics import f1_score# bayesian optimizationfrom bayes_opt import BayesianOptimization# base modelsfrom sklearn.ensemble import RandomForestClassifierfrom lightgbm import LGBMClassifierfrom sklearn.svm import LinearSVCfrom sklearn.neural_network import MLPClassifier# second layer modelfrom xgboost import XGBClassifierFor large data sets, tuning each base model can take a very long time. To prevent any accidental failure, I recommend saving the models (or to be more specific, the parameters of the models) immediately when the tuning is done.To save and load your model, you can use sklearn’s joblib.# save modeljoblib.dump(model, filename)# load modeljoblib.load(filename)from sklearn.externals import joblibfrom time import gmtime, strftimeimport json# parameters for bayesian optimizationgp_params = {"alpha": 1e-5}n_iter=5# Function to generate file name based completion timedef generate_filename(model):    return str(model)+strftime("%Y-%m-%d_%H:%M", gmtime())+'.pkl'# Function to save the log of parameter tuningdef BO_log(BO, name):    fname = name + strftime("%Y-%m-%d_%H:%M", gmtime()) + '.json'    with open(fname, 'w') as fp:        json.dump(BO.res, fp)    print(fname + ' saved')Random ForestSince we are dealing with imbalanced data, we need to be careful in model training. In the past, my colleagues and I have tried every popular sampling methods (over-sampling, under-sampling, and SMOTE) but in the end we decided to adjust the class weight directly. There are a few advantages of this method: 1) it is easy to do; 2) it does not make any assumptions on the distribution of the features; 3) when it is not clear what weight you should assign to each class, you can treat it as a hyper-parameter and let machine do the work for you.Regarding the metrics used for optimization, I am using f1 score since this was the judging criteria of the competition I participated in. Of course you can use any metric you wish, but keep in mind that when you are dealing imbalanced data, you have to be careful with your choice.def rfc(n_estimators, min_samples_split, max_depth, class_weight):    return RandomForestClassifier(n_estimators=int(n_estimators),            min_samples_split=int(min_samples_split),            max_depth=int(max_depth),            random_state=42,            n_jobs=-1,            bootstrap=False,            class_weight={0:1, 1:class_weight}        )def rfccv(**params):    val = cross_val_score(        rfc(**params),        x_train, y_train, scoring='f1', cv=5    ).mean()    return valrfcBO = BayesianOptimization(        rfccv,        {'n_estimators': (50, 500),        'min_samples_split': (2, 25),        'max_depth': (3, 12),        'class_weight': (4, 10)        }    )rfcBO.maximize(n_iter=n_iter, **gp_params)BO_log(rfcBO, 'rfc')rf_clf = rfc(**rfcBO.res['max']['max_params'])joblib.dump(rf_clf, generate_filename('rf_clf'))rf_clf.fit(x_train, y_train)rf_pred = rf_clf.predict(x_test)print(f1_score(y_test, rf_pred))rf_oof_train, rf_oof_test = get_oof(rf_clf, x_train, y_train, x_test)LightGBMdef lgb(n_estimators, learning_rate, scale_pos_weight):    return LGBMClassifier(n_estimators=int(n_estimators),            learning_rate=learning_rate,            scale_pos_weight=scale_pos_weight,            seed=42        )def lgbcv(**params):    val = cross_val_score(        lgb(**params),        x_train, y_train, scoring='f1', cv=5    ).mean()    return vallgbcBO = BayesianOptimization(        lgbcv,        {'n_estimators': (50, 500),         'learning_rate': (0.01, 0.1),         'scale_pos_weight': (4, 10)        }    )lgbcBO.maximize(n_iter=n_iter, **gp_params)BO_log(lgbcBO, 'lgb')lgb_clf = lgb(**lgbcBO.res['max']['max_params'])joblib.dump(lgb_clf, generate_filename('lgb_clf'))lgb_clf.fit(x_train, y_train)lgb_pred = lgb_clf.predict(x_test)print(f1_score(y_test, lgb_pred))lgb_oof_train, lgb_oof_test = get_oof(lgb_clf, x_train, y_train, x_test) SVMWe need to normalize data first before training SVM and MLP.from sklearn.preprocessing import StandardScalerx_train_scaled = StandardScaler().fit_transform(x_train)x_test_scaled = StandardScaler().fit_transform(x_test)def linsvc(C, class_weight):    return LinearSVC(C=C,            random_state=42,            class_weight={0:1, 1:class_weight}        )def linsvccv(**params):    val = cross_val_score(        linsvc(**params),        x_train_scaled, y_train, scoring='f1', cv=5, n_jobs=-1    ).mean()    return vallinsvcBO = BayesianOptimization(        linsvccv,        {'C': (1, 10000),         'class_weight': (4, 10)}    )linsvcBO.maximize(n_iter=n_iter, **gp_params)BO_log(linsvcBO, 'linsvc')linsvc_clf = linsvc(**linsvcBO.res['max']['max_params'])joblib.dump(linsvc_clf, generate_filename('linsvc_clf'))linsvc_clf.fit(x_train_scaled, y_train)linsvc_pred = linsvc_clf.predict(x_test_scaled)print(f1_score(y_test, linsvc_pred))linsvc_oof_train, linsvc_oof_test = get_oof(linsvc_clf,x_train_scaled, y_train, x_test_scaled)MLPdef mlp(first_layer, second_layer, alpha):    return MLPClassifier(            hidden_layer_sizes=(int(first_layer), int(second_layer)),            alpha=10**alpha        )def mlpccv(**params):    val = cross_val_score(        mlp(**params),        x_train_scaled, y_train, scoring='f1', cv=5, n_jobs=-1    ).mean()    return valmlpcBO = BayesianOptimization(        mlpccv,        {'first_layer': (4, 8),         'second_layer': (2, 4),         'alpha': (-5, -2)}    )mlpcBO.maximize(n_iter=n_iter, **gp_params)BO_log(mlpcBO, 'mlp')mlp_clf = mlp(**mlpcBO.res['max']['max_params'])joblib.dump(mlp_clf, generate_filename('mlp_clf'))mlp_clf.fit(x_train_scaled, y_train)mlp_pred = mlp_clf.predict(x_test_scaled)print(f1_score(y_test, mlp_pred))mlp_oof_train, mlp_oof_test = get_oof(mlp_clf, x_train_scaled, y_train, x_test_scaled) Stacking Base Modelsbase_predictions_train = pd.DataFrame(    {'RandomForest': rf_oof_train.ravel(),     'LightGBM': lgb_oof_train.ravel(),     'LinSVC': linsvc_oof_train.ravel(),     'MLP': mlp_oof_train.ravel(),    })base_predictions_train.head()                  LightGBM      LinSVC      MLP      RandomForest                  0      0.0      0.0      0.0      0.0              1      0.0      0.0      0.0      0.0              2      0.0      0.0      0.0      0.0              3      0.0      0.0      0.0      0.0              4      0.0      0.0      0.0      0.0      sns.heatmap(base_predictions_train.astype(float).corr().values,            xticklabels=base_predictions_train.columns,            yticklabels=base_predictions_train.columns,            annot=True)This heat map was taken from one of the randomly generated data I have run. In general, as mentioned by Džeroski and Ženko, it is a good idea to include heterogenous classifiers as base models for stacking, i.e. classifiers that use different model representations. Since both random forest and lightgbm are tree-based, their high correlation is expected.xg_train = np.concatenate((rf_oof_train,                            lgb_oof_train,                            linsvc_oof_train,                            mlp_oof_train,                           ), axis=1)xg_test = np.concatenate((rf_oof_test,                           lgb_oof_test,                           linsvc_oof_test,                           mlp_oof_test,                          ), axis=1)def xgb(max_depth,           learning_rate,           n_estimators,           gamma,           min_child_weight,           subsample,          scale_pos_weight):    return XGBClassifier(            max_depth=int(max_depth),            learning_rate=learning_rate,            n_estimators=int(n_estimators),            min_child_weight=int(min_child_weight),            gamma=max(gamma, 0),            subsample=max(min(subsample, 1), 0),            scale_pos_weight=scale_pos_weight,            random_state=42,            n_jobs=-1        )def xgbcv(**params):    val = cross_val_score(        xgb(**params),        xg_train, y_train, scoring='f1', cv=5    ).mean()    return valxgbBO = BayesianOptimization(        xgbcv,        {'max_depth': (3, 10),         'learning_rate': (0.05, 0.3),         'n_estimators':(50, 500),         'min_child_weight':(1, 20),         'gamma':(0, 10),         'subsample':(0.5, 1),         'scale_pos_weight':(2, 10)        }    )xgbBO.maximize(n_iter=n_iter, **gp_params)BO_log(xgbBO, 'xgb')xgb_clf = xgb(**xgbBO.res['max']['max_params'])joblib.dump(xgb_clf, generate_filename('xgb_clf'))xgb_clf.fit(xg_train, y_train)xgb_pred = xgb_clf.predict(xg_test)print(f1_score(y_test, xgb_pred))ConclusionThe aim of this template is to provide a quick set up of a stacking ensemble with bayesian optimization. Based on my work experience in fraud detection, finding the right features is much more important than building a fancy model. I believe that for most machine learning applications, the key to create the best model is thorough understanding of the problem so that one could figure out features that best describes the situation. I hope this template can save you some time in getting the model to work so that you can stay focused on the problem per se.]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> python </tag>
        
          <tag> machine learning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[3D Interactive Visualization of Graph]]></title>
      <url>/2017/11/07/3D-Interactive-Visualization-of-Graph/</url>
      <content type="text"><![CDATA[Required PackagesThis short demo uses the following packages: plotly, networkx, and igraph, which can be installed with pip.Building Graph from Raw DataData PreviewI am using a sample data set generated by random numbers. By the way, I have uploaded sample data and ipython notebook of this demo here.df = pd.read_csv('sample.csv')df.head()                  receiver      sender      data_received                  0      71088000517510301092      617405214587577481      632              1      71088000517510301092      7884137335649481972584      76715              2      71088000517510301092      7884137335649481972584      51591              3      71088000517510301092      2714752644432115202      33814              4      71088000517510301092      7884137335649481972584      5651      In the dataset, receiver and sender columns contain the id of the sender and receiver in a network, while data received is the amount of data receiver received from the sender.Building GraphFor simple illustration, I am building a non-directed graph using the number of connections between sender and receiver as weight. I removed edges with weight 1 since they are not very interesting.# Data Preparationdf_id = df[['receiver', 'sender']]df_id = df_id.groupby(['receiver', 'sender']).agg({'sender':'count'})df_id.columns = ['count']df_id.reset_index(inplace=True)# Build GraphG = nx.Graph()mat_id = df_id.valuesfor i in range(0, mat_id.shape[0]):    if mat_id[i, 2] &gt; 1:        G.add_edge(mat_id[i, 0], mat_id[i, 1], weight=mat_id[i, 2])Here we will save G to gml file for later use.from networkx.readwrite import gmlgml.write_gml(G, 'graph.gml')Graph AnalysisWe use the louvian algorithm to perform some simple community discovery on the graph we just built.import communitypartition = community.best_partition(G)df_partition = pd.DataFrame.from_dict(partition, orient='index')# reformat partition for later usedf_partition.reset_index(inplace=True)df_partition.columns = [['id', 'group']]df_group = pd.DataFrame(df_partition.groupby('group').apply(lambda x: ','.join(x.id)))group_list = []for members in df_group[0]:    group_list.append(frozenset(members.split(',')))VisualizationWe will need iGraph and plotly for 3D visualization. Note that we need to run plotly in offline mode.import igraph as igimport plotlyimport plotly.offline as offlineimport plotly.graph_objs as gooffline.init_notebook_mode()We need to load the graph G to an iGraph object, which can be easily done by loading the gml file we have saved.F = ig.Graph.Read_GML('graph.gml')Before getting ready to draw the graph, we need to obtain the 3D coordinate of the nodes, and here is why I am using iGraph instead of networkx, since networkx has a poor support for 3D graph. For simplicity, I followed the instructions on plotly official website, which can be found herelayt = F.layout('kk', dim=3)Edges = []for e in F.es:    Edges.append(e.tuple)labels=[]group=[]for node in F.vs['label']:    labels.append(node)    group.append(int(df_partition[df_partition['id']==node]['group']))N = len(labels)Xn=[layt[k][0] for k in range(N)]# x-coordinates of nodesYn=[layt[k][1] for k in range(N)]# y-coordinatesZn=[layt[k][2] for k in range(N)]# z-coordinatesXe=[]Ye=[]Ze=[]for e in Edges:    Xe+=[layt[e[0]][0],layt[e[1]][0], None]# x-coordinates of edge ends    Ye+=[layt[e[0]][1],layt[e[1]][1], None]    Ze+=[layt[e[0]][2],layt[e[1]][2], None]import plotly.plotly as pyfrom plotly.graph_objs import *trace1=Scatter3d(x=Xe,               y=Ye,               z=Ze,               mode='lines',               line=Line(color='rgb(125,125,125)', width=1),               hoverinfo='none'               )trace2=Scatter3d(x=Xn,               y=Yn,               z=Zn,               mode='markers',               name='id',               marker=Marker(symbol='dot',                             size=6,                             color=group,                             colorscale='jet',                             line=Line(color='rgb(50,50,50)', width=0.5)                             ),               text=labels,               hoverinfo='text'               )axis=dict(showbackground=False,          showline=False,          zeroline=False,          showgrid=False,          showticklabels=False,          title=''          )layout = Layout(         title="Network of Communications Weighted by Connection (3D visualization)",         width=1000,         height=1000,         showlegend=False,         scene=Scene(         xaxis=XAxis(axis),         yaxis=YAxis(axis),         zaxis=ZAxis(axis),        ),     margin=Margin(        t=100    ),    hovermode='closest',    annotations=Annotations([           Annotation(           showarrow=False,               text="",            xref='paper',            yref='paper',            x=0,            y=0.1,            xanchor='left',            yanchor='bottom',            font=Font(            size=14            )            )        ]),    )data=Data([trace1, trace2])fig=Figure(data=data, layout=layout)offline.iplot(fig, filename='Network')]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> python </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Using Jupyter Notebook on Server]]></title>
      <url>/2017/11/06/Using-Jupyter-Notebook-Remotely/</url>
      <content type="text"><![CDATA[Why Use Jupyter Notebook on ServerWhen you are working on large data set or need extra computing power, you may need to do the work remotely on a server. For students, since you have access to AWS educate, you should definitely take advantage of it.StepsOn the ServerFirst, you may want to set up a password for your notebook. For detailed instruction, you can look up here.If you are looking for a quick set up, you can follow these steps:jupyter notebook --generate-configThen you can create your password byjupyter notebook passwordAfter setting up password, start a jupyter notebook in no browser mode, and specify a port you like. For example,jupyter notebook --no-browser --port=8889Of course, you can replace 8889 with any port you like, just make sure there is no clash.If you plan to run the notebook for long period of time, you can run your notebook without hangupnohup jupyter notebook --no-browser --port=8889In this way, your notebook will keep running even if you log out of the server.On Local MachineNow you need to connect to your notebook from your local machine.You need to create a ssh tunnel to your server and bind your notebook port to a specified local port. Continuing from the previous example, on your local machine,ssh [USER]@[SERVER] -L 8889:localhost:8888 -Nwhere [USER] is your user name, and [SERVER] is your server’s address.Now you can launch jupyter notebook as usual, by typing ‘localhost:8888’ in your favorite browser.To Kill the Jupyter Notebok ProcessSince we launched jupyter notebook with nohup, you can do the following to kill the jupyter notebook process.netstat -tulpnThen kill the pid with python.]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> jupyter </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
